# -*- coding: utf-8 -*-
"""240322 Improved Deep Voting model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SMx_uswivwpXztaR71OVRoxXvPvnc7op

#Recall the Data
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision.datasets as datasets
import torchvision.transforms as transforms
import random
from torch.utils.data import Dataset, DataLoader
import pandas as pd
!pip install pandas numpy scikit-learn tensorflow
import pandas as pd
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np

print(torch.__version__)

from google.colab import drive
drive.mount('/content/drive')

finaldata = pd.read_csv("/content/drive/MyDrive/capstone_yb/Voting ML/DATA/cleaned_finaldata.csv")

X = finaldata.drop(columns=['Geography', 'Geographic Area Name','Biden_proportion','Estimate!!Households!!Median income (dollars)','Vote Count', 'Precinct','County',
                            'Estimate!!Families!!Median income (dollars)','Estimate!!Nonfamily households!!Median income (dollars)', 'Estimate!!Married-couple families!!Median income (dollars)'])
X = X.astype(float)
X.dtypes

y = finaldata['Biden_proportion']

import copy
import numpy as np
import tqdm
from sklearn.model_selection import train_test_split


# train-test split of the dataset / chaning split of data to Pytorch
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)
X_train = torch.tensor(X_train.values, dtype=torch.float32)
y_train = torch.tensor(y_train.values, dtype=torch.float32).reshape(-1, 1)
X_test = torch.tensor(X_test.values, dtype=torch.float32)
y_test = torch.tensor(y_test.values, dtype=torch.float32).reshape(-1, 1)

print(X_train.shape)
print(y_train.shape)

"""#Increase the number of layers+ Change the size of each layer"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

"""Neural Network Architecture:

Original Model (SimpleNN): It includes two linear layers (an input and an output layer) with ReLU activation function. This structure is relatively simple and performs basic feature extraction.

Improved Model (DeepNN): This model increases depth by adding multiple hidden layers. This allows the model to learn more complex patterns and generally improves prediction performance.
"""

# Define the neural network class
'''class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(20, 32)
        self.fc2 = nn.Linear(32, 1)
        self.relu = nn.ReLU()''from sklearn.model_selection import train_test_split'''

class DeepNN(nn.Module):
    def __init__(self):
        super(DeepNN, self).__init__()
        self.fc1 = nn.Linear(20, 32)  # Input layer
        self.bn1 = nn.BatchNorm1d(32)  # Batch normalization after first linear layer
        self.fc2 = nn.Linear(32, 64)  # First hidden layer
        self.bn2 = nn.BatchNorm1d(64)  # Batch normalization after second linear layer
        self.dropout = nn.Dropout(0.5)  # Dropout for regularization
        self.fc3 = nn.Linear(64, 64)  # Second hidden layer
        self.bn3 = nn.BatchNorm1d(64)  # Batch normalization after third linear layer
        self.fc4 = nn.Linear(64, 1)    # Output layer
        self.relu = nn.ReLU()  # ReLU activation used throughout

    def forward(self, x):
        x = self.relu(self.bn1(self.fc1(x)))  # Activation -> BatchNorm
        x = self.relu(self.bn2(self.fc2(x)))  # Activation -> BatchNorm
        x = self.dropout(x)  # Applying dropout after activation
        x = self.relu(self.bn3(self.fc3(x)))  # Activation -> BatchNorm
        x = self.fc4(x)  # Output layer does not need activation if regression
        return x

# Instantiate the model
model = DeepNN()

county_info = finaldata['County'].unique().tolist()
len(county_info)

class CountyDataset(Dataset): #classcification
    def __init__(self, finaldata):
        f =  finaldata.drop(columns=['Geography', 'Geographic Area Name','Biden_proportion','Estimate!!Households!!Median income (dollars)','Vote Count', 'Precinct','County',
                            'Estimate!!Families!!Median income (dollars)','Estimate!!Nonfamily households!!Median income (dollars)', 'Estimate!!Married-couple families!!Median income (dollars)']).astype(float)
        self.X = f.values.tolist()
        self.y = finaldata['Biden_proportion'].tolist()
        self.county_info = finaldata['County'].tolist()

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.float32), self.county_info[idx]

dataset = CountyDataset(finaldata)

class CountyDataLoader: #load the data with county
    def __init__(self, dataset, random_state=None):
        self.dataset = dataset
        self.counties = list(set(dataset.county_info))
        self.random_state = random_state

    def __iter__(self): #load the data with each county
        for county in self.counties:
            county_indices = [i for i, c in enumerate(self.dataset.county_info) if c == county]
            batch_X = [self.dataset.X[i] for i in county_indices]
            batch_y = [self.dataset.y[i] for i in county_indices]
            yield batch_X, batch_y

county_dataloader = CountyDataLoader(dataset)

# Define the custom loss function
def custom_loss(predictions, county_statistics):
    # Calculate the average prediction within the batch
    avg_prediction = torch.mean(predictions)

    # Assume county_statistics is a tuple containing the required statistics (e.g., Biden proportion)
    biden_proportion = county_statistics[0]

    # Calculate the MSE loss(Mean Squared Error)
    mse_loss = nn.MSELoss()(avg_prediction, biden_proportion)

    return mse_loss

# Define the loss function
class LossFunction(nn.Module):
    def __init__(self):
        super(LossFunction, self).__init__()

    def forward(self, predictions, mean_l): #calculate MSE
        mean_p = torch.mean(predictions)
        mean_l = torch.tensor(mean_l, dtype=torch.float32)

        return torch.square(mean_p - mean_l)

"""#Same Optimizer: SGD"""

optimizer = torch.optim.SGD(model.parameters(), lr=0.001) #SGD optimizer
import torch
num_epochs = 500
losses = []
for epoch in range(num_epochs):
    for i,(train_X, train_y)  in enumerate(county_dataloader):
      epoch_loss = 0.0
      if i<32:

        # Convert lists to PyTorch tensors
        features = torch.tensor(train_X)
        target = torch.tensor(train_y)

        # Forward pass, loss computation, and backward pass
        outputs = model(features)
        # print(outputs) # issue here
        loss = custom_loss(outputs, target)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
        average_epoch_loss = epoch_loss /50
       # losses.append(loss.item())
        losses.append(average_epoch_loss)

    if epoch % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')

import matplotlib.pyplot as plt

plt.plot(losses[10:], label='Training Loss') #The code plots only the data points for the training loss after excluding the first 10 epochs.
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

import numpy as np
import time

rmse_total = 0
num_batches = 0
losses = []
timestamps = []
epoch_test_loss = 0.0

for i, (val_X, val_y) in enumerate(county_dataloader):
    if i >= 32:
        start_time = time.time()
        var = 0.0

        features = torch.tensor(val_X)
        target = torch.tensor(val_y)

        outputs = model(features)
        loss = custom_loss(outputs, target)

        rmse_total += loss.item()
        var += loss.item()
        avg = var / 12
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        num_batches += 1
        end_time = time.time()
        losses.append(avg)
        timestamps.append(end_time - start_time)

average_rmse = np.sqrt(rmse_total / max(num_batches, 1))

print(f'Average RMSE: {average_rmse, num_batches}')

"""Original(Simple NN+ SGD) RMSE: Average RMSE: (0.27092237316695306, 7)

#Use a different optimizer (ADAM, ADAGRAD)

MY prediction: To improve the model's performance, we'll switch to using the Adam optimizer. Adam is well-regarded for its effectiveness across a wide range of deep learning models and provides a good starting point for many problems. One of the key advantages of Adam is its ability to automatically adjust the learning rate during training, which often leads to better performance in comparison to optimizers that keep the learning rate constant.

SGD(previous) is a simpler and older optimization technique that has been extensively used due to its straightforward implementation and reliable performance across various types of problems. However, in complex scenarios involving deep learning, Adam is often preferred because of its robustness and ease of use, particularly in settings where choosing the right learning rate is critical for achieving good results.

##ADAM
"""

optimizer2 = optim.Adam(model.parameters(), lr=0.001) #New: SGD ->> ADAM
#optimizer = torch.optim.SGD(model.parameters(), lr=0.001) : Original
#optimizer = optim.Nadam(model.parameters(), lr=0.001)
#optimizer = optim.RMSprop(model.parameters(), lr=0.001)

import torch
num_epochs = 500
losses = []
for epoch in range(num_epochs):
    for i,(train_X, train_y)  in enumerate(county_dataloader):
      epoch_loss = 0.0
      if i<32:

        # Convert lists to PyTorch tensors
        features = torch.tensor(train_X)
        target = torch.tensor(train_y)

        # Forward pass, loss computation, and backward pass
        outputs = model(features)
        # print(outputs) # issue here
        loss = custom_loss(outputs, target)
        optimizer2.zero_grad()
        loss.backward()
        optimizer2.step()
        epoch_loss += loss.item()
        average_epoch_loss = epoch_loss /50
       # losses.append(loss.item())
        losses.append(average_epoch_loss)

    if epoch % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')

plt.plot(losses[10:], label='Training Loss') #The code plots only the data points for the training loss after excluding the first 10 epochs.
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

optimizer = torch.optim.SGD(model.parameters(), lr=0.001)

import torch
num_epochs = 500
losses = []
for epoch in range(num_epochs):
    for i,(train_X, train_y)  in enumerate(county_dataloader):
      epoch_loss = 0.0
      if i<32:

        # Convert lists to PyTorch tensors
        features = torch.tensor(train_X)
        target = torch.tensor(train_y)

        # Forward pass, loss computation, and backward pass
        outputs = model(features)
        # print(outputs) # issue here
        loss = custom_loss(outputs, target)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
        average_epoch_loss = epoch_loss /50
       # losses.append(loss.item())
        losses.append(average_epoch_loss)

    if epoch % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')

plt.plot(losses[10:], label='Training Loss') #The code plots only the data points for the training loss after excluding the first 10 epochs.
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""We switched to the Adam optimizer to enhance the model's performance, touted for its efficacy across a broad spectrum of deep learning models and its capability to automatically adjust the learning rate during training. However, actual experimental results indicated that using Adam led to unstable and inconsistent loss values. Despite Adam's advantages, such as adapting the learning rate, it does not always guarantee stable convergence in the training process.

On the other hand, SGD (Stochastic Gradient Descent), though simpler and more traditional, proved to be more stable in reducing loss during our experiments. This stability underscores that SGD remains a viable option even in complex deep learning scenarios. The automatic adjustment of the learning rate by Adam can be beneficial, but it does not universally translate to the best outcome in every situation.

These observations suggest that the choice of optimizer significantly impacts model performance, and the optimal optimizer may vary depending on the specific data or model architecture. Therefore, it is crucial to experiment with various optimizers under different settings and conditions to derive the best results.

In summary, the text advocates for the use of the Adam optimizer in complex deep learning tasks due to its adaptive learning rate capabilities, which generally yield better performance compared to the traditional SGD optimizer.
"""

model.eval()

import numpy as np
import time

rmse_total = 0
num_batches = 0
losses = []
timestamps = []
epoch_test_loss = 0.0

for i, (val_X, val_y) in enumerate(county_dataloader):
    if i >= 32:
        start_time = time.time()
        var = 0.0

        features = torch.tensor(val_X)
        target = torch.tensor(val_y)

        outputs = model(features)
        loss = custom_loss(outputs, target)

        rmse_total += loss.item()
        var += loss.item()
        avg = var / 12
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        num_batches += 1
        end_time = time.time()
        losses.append(avg)
        timestamps.append(end_time - start_time)

average_rmse = np.sqrt(rmse_total / max(num_batches, 1))

print(f'Average RMSE: {average_rmse, num_batches}') #Average RMSE: (0.27092237316695306, 7)

# Plotting the loss versus time for both training and test data
plt.plot(timestamps, losses, marker='o')
plt.xlabel('Time (seconds)')
plt.ylabel('Loss')
plt.title('Loss Versus Time for Test Data')

"""##Evaluation"""

merged_df = pd.read_csv("/content/drive/MyDrive/processed_data.csv")

class CountyDataset2(Dataset):
    def __init__(self, finaldata):
        f =  finaldata.drop(columns=['Geography', 'Geographic Area Name','Precinct','County', 'Vote Count','Estimate!!Households!!Median income (dollars)','average_Biden_proportion',
                            'Estimate!!Families!!Median income (dollars)','Estimate!!Nonfamily households!!Median income (dollars)', 'Estimate!!Married-couple families!!Median income (dollars)']).astype(float)
        self.X = f.values.tolist()
        self.y = finaldata['average_Biden_proportion'].tolist()
        self.county_info = finaldata['County'].tolist()

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.float32), self.county_info[idx]

dataset_evaluation = CountyDataset2(merged_df)
county_dataloader2 = CountyDataLoader(dataset_evaluation)

rmse_total = 0
num_batches = 0
losses1 = []
timestamps1 = []
for i,(val_X, val_y) in enumerate(county_dataloader2):
    var = 0.0
    start_time = time.time()


    features = torch.tensor(val_X)
    target = torch.tensor(val_y)


    outputs = model(features)
    loss = custom_loss(outputs, target)

    rmse_total += loss.item()
    var += loss.item()
    avg = var/62
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    end_time = time.time()

    num_batches += 1
    losses1.append(avg)
    timestamps1.append(end_time - start_time)

average_rmse = np.sqrt(rmse_total / num_batches)

print(f'Average RMSE: {average_rmse,num_batches }') #Average RMSE: (0.17563571724326635, 38)

# Plotting the loss versus time
plt.plot(timestamps1, losses1, marker='o')
plt.xlabel('Time (seconds)')
plt.ylabel('Loss')
plt.title('Loss Versus Time for Evaluation Data')
plt.show()